- title: "Where should cameras look at soccer games: improving smoothness using the overlapped hidden Markov model"
  image: dummy.png
  description: "Automatic camera planning for sports has been a long term goal in computer vision and machine learning. In this paper, we study camera planning for soccer games using pan, tilt and zoom (PTZ) cameras. Two important problems have been addressed. First, we propose the Overlapped Hidden Markov Model (OHMM) method which effectively optimizes the camera trajectory in overlapped local windows. The OHMM method significantly improves the smoothness of the camera planning by optimizing the camera trajectory in the temporal space, resulting in much more natural camera movements present in real broadcasts. We also propose CalibMe which is a highly automatic camera calibration method for soccer games. CalibMe enables users to collect large amounts of training data for learning algorithms. The precision of CalibMe is evaluated on a motion blur affected sequence and outperforms several strong existing methods. The performance of the OHMM method is extensively evaluated on both synthetic and real data. It outperforms the state-of-the-art algorithms in terms of smoothness without sacrificing accuracy."
  authors: J Chen and J J. Little
  link:
    url: https://www.sciencedirect.com/science/article/pii/S1077314216301709
    display:  Compuer Vision and Image Understanding (2017)
  highlight: 0
  news2: 

- title: "Play and Learn: Using Video Games to Train Computer Vision Models"
  image: bmvc16_1.png
  description: Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.
  authors: A. Shafaei, J. J. Little, Mark Schmidt
  link:
    url: http://www.bmva.org/bmvc/2016/papers/paper026/index.html
    display: BMVC (2016)
  highlight: 1
  news2: 

- title: "Real-Time Human Motion Capture with Multiple Depth Cameras"
  image: crv16_1.png
  description: Commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras. In this work we present an efficient and inexpensive solution to markerless motion capture using only a few Kinect sensors. Unlike the previous work on 3d pose estimation using a single depth camera, we relax constraints on the camera location and do not assume a co-operative user. We apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data. Our method accurately localizes body parts without requiring an explicit shape model. The body joint locations are then recovered by combining evidence from multiple views in real-time. We also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the Berkeley MHAD dataset.
  authors: A. Shafaei, J. J. Little
  link:
    url: http://www.cs.ubc.ca/~shafaei/homepage/projects/papers/crv_16.pdf
    display: CRV (2016)
  highlight: 1
  news2: 

- title: "Learning Online Smooth Predictions for Realtime Camera Planning  using Recurrent Decision Trees"
  image: dummy.png
  description: "We study the problem of online prediction for realtime camera planning, where the goal is to predict smooth trajectories that correctly track and frame objects of interest (e.g., players in a basketball game). The conventional approach for training predictors does not directly consider temporal consistency, and often produces undesirable jitter. Although post-hoc smoothing (e.g., via a Kalman filter) can mitigate this issue to some degree, it is not ideal due to overly stringent modeling assumptions (e.g., Gaussian noise). We propose a recurrent decision tree framework that can directly incorporate temporal consistency into a data-driven predictor, as well as a learning algorithm that can efficiently learn such temporally smooth models. Our approach does not require any post-processing, making online smooth predictions much easier to generate when the noise model is unknown. We apply our approach to sports broadcasting: given noisy player detections, we learn where the camera should look based on human demonstrations. Our experiments exhibit significant improvements over conventional baselines and showcase the practicality of our approach."
  authors:  J Chen, H M. Le. P Carr, Y Yue, J J. Little
  link:
    url: http://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Learning_Online_Smooth_CVPR_2016_paper.pdf
    display:  Computer Vision and Pattern Recognition (2016)
  highlight: 0
  news2: 

- title: "Unlabelled 3D Motion Examples Improve Cross View Action Recognition"
  image: dummy.png
  description: "We demonstrate a novel strategy for unsupervised cross view action recognition using multi view feature synthesis. We do not rely on cross view video annotations to transfer knowledge across views but use local features generated using motion capture data to learn the feature transformation. Motion capture data allows us to build a feature level correspondence between two synthesized views. We learn a feature mapping scheme for each view change by making a naive assumption that all features transform independently. This assumption along with the exact feature correspondences dramatically simplifies learning. With this learned mapping we are able to hallucinate action descriptors corresponding to different viewpoints. This simple approach effectively models the transformation of BoW based action descriptors under viewpoint change and outperforms the state of the art on the INRIA IXMAS dataset."
  authors:  A. Gupta, A. Shafaei, J. J. Little and R. J. Woodham
  link:
    url: http://www.cs.ubc.ca/~shafaei/homepage/projects/papers/bmvc_14.pdf
    display:  BMVC (2014)
  highlight: 0
  news2: 